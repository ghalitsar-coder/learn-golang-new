package main

import (
	"fmt"
	"runtime"
	"sync"
	"time"
)

// APPROACH 1: One Goroutine Per Job (TIDAK DIREKOMENDASIKAN)
func oneGoroutinePerJob(numJobs int) {
	fmt.Printf("\nüî• APPROACH 1: One Goroutine Per Job (%d jobs = %d goroutines)\n", numJobs, numJobs)
	fmt.Println("=" + fmt.Sprintf("%60s", "="))

	start := time.Now()
	var wg sync.WaitGroup

	// Catat goroutine sebelum
	goroutinesBefore := runtime.NumGoroutine()

	for i := 0; i < numJobs; i++ {
		wg.Add(1)
		go func(jobID int) {
			defer wg.Done()

			// Simulasi pekerjaan
			time.Sleep(100 * time.Millisecond)
			fmt.Printf("Job %d selesai\n", jobID)
		}(i)
	}

	// Catat puncak goroutine
	maxGoroutines := runtime.NumGoroutine()

	wg.Wait()
	duration := time.Since(start)

	fmt.Printf("\nüìä RESULTS:\n")
	fmt.Printf("   ‚è±Ô∏è  Duration: %v\n", duration)
	fmt.Printf("   üßµ Goroutines before: %d\n", goroutinesBefore)
	fmt.Printf("   üßµ Peak goroutines: %d\n", maxGoroutines)
	fmt.Printf("   üíæ Memory overhead: ~%d KB (estimasi)\n", (maxGoroutines-goroutinesBefore)*8)
}

// APPROACH 2: Worker Pool Pattern (DIREKOMENDASIKAN)
func workerPoolPattern(numJobs int, numWorkers int) {
	fmt.Printf("\n‚úÖ APPROACH 2: Worker Pool (%d jobs dengan %d workers)\n", numJobs, numWorkers)
	fmt.Println("=" + fmt.Sprintf("%60s", "="))

	start := time.Now()

	// Channel untuk jobs
	jobs := make(chan int, numJobs)
	results := make(chan string, numJobs)

	var wg sync.WaitGroup

	// Catat goroutine sebelum
	goroutinesBefore := runtime.NumGoroutine()

	// Start workers (fixed number)
	for w := 1; w <= numWorkers; w++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()

			for job := range jobs {
				// Simulasi pekerjaan
				time.Sleep(100 * time.Millisecond)
				result := fmt.Sprintf("Worker-%d completed job-%d", workerID, job)
				results <- result
			}
		}(w)
	}

	// Send jobs
	for i := 0; i < numJobs; i++ {
		jobs <- i
	}
	close(jobs)

	// Catat puncak goroutine
	maxGoroutines := runtime.NumGoroutine()

	// Wait for workers to complete
	wg.Wait()
	close(results)

	// Collect results
	var resultCount int
	for result := range results {
		_ = result // Process result (simplified)
		resultCount++
	}

	duration := time.Since(start)

	fmt.Printf("\nüìä RESULTS:\n")
	fmt.Printf("   ‚è±Ô∏è  Duration: %v\n", duration)
	fmt.Printf("   üßµ Goroutines before: %d\n", goroutinesBefore)
	fmt.Printf("   üßµ Peak goroutines: %d\n", maxGoroutines)
	fmt.Printf("   üíæ Memory overhead: ~%d KB (estimasi)\n", (maxGoroutines-goroutinesBefore)*8)
	fmt.Printf("   ‚úÖ Jobs completed: %d\n", resultCount)
}

// DEMO: Memory dan Performance Impact
func demonstrateScalabilityIssues() {
	fmt.Println("\nüö® DEMO: SCALABILITY ISSUES dengan Many Goroutines")
	fmt.Println("=" + fmt.Sprintf("%60s", "="))

	jobs := []int{10, 100, 1000, 5000}

	for _, numJobs := range jobs {
		fmt.Printf("\n--- Testing dengan %d jobs ---\n", numJobs)

		// Test 1: One Goroutine Per Job
		start := time.Now()
		var wg sync.WaitGroup

		goroutinesBefore := runtime.NumGoroutine()

		for i := 0; i < numJobs; i++ {
			wg.Add(1)
			go func(jobID int) {
				defer wg.Done()
				time.Sleep(10 * time.Millisecond) // Pekerjaan ringan
			}(i)
		}

		maxGoroutines := runtime.NumGoroutine()
		wg.Wait()
		duration1 := time.Since(start)

		// Test 2: Worker Pool (optimal workers = CPU cores)
		numWorkers := runtime.NumCPU()
		start = time.Now()

		jobs := make(chan int, numJobs)
		var wg2 sync.WaitGroup

		// Start workers
		for w := 0; w < numWorkers; w++ {
			wg2.Add(1)
			go func() {
				defer wg2.Done()
				for range jobs {
					time.Sleep(10 * time.Millisecond)
				}
			}()
		}

		// Send jobs
		for i := 0; i < numJobs; i++ {
			jobs <- i
		}
		close(jobs)

		wg2.Wait()
		duration2 := time.Since(start)

		fmt.Printf("üî• One-per-job: %v, %d goroutines\n", duration1, maxGoroutines-goroutinesBefore)
		fmt.Printf("‚úÖ Worker pool: %v, %d workers\n", duration2, numWorkers)

		// Calculate efficiency
		efficiency := float64(duration1.Nanoseconds()) / float64(duration2.Nanoseconds())
		if duration2 < duration1 {
			fmt.Printf("üí° Worker pool %0.1fx lebih efisien!\n", efficiency)
		} else if duration1 < duration2 {
			fmt.Printf("‚ö†Ô∏è  One-per-job %0.1fx lebih cepat (overhead kecil)\n", 1/efficiency)
		}
	}
}

// DEMO: Resource Management
func demonstrateResourceManagement() {
	fmt.Println("\nüíæ DEMO: RESOURCE MANAGEMENT")
	fmt.Println("=" + fmt.Sprintf("%50s", "="))

	numJobs := 1000

	fmt.Println("\n1Ô∏è‚É£ Sebelum membuat goroutines:")
	var m1 runtime.MemStats
	runtime.ReadMemStats(&m1)
	goroutines1 := runtime.NumGoroutine()
	fmt.Printf("   Memory: %d KB, Goroutines: %d\n", m1.Alloc/1024, goroutines1)

	// Create many goroutines
	var wg sync.WaitGroup
	for i := 0; i < numJobs; i++ {
		wg.Add(1)
		go func(id int) {
			defer wg.Done()
			time.Sleep(50 * time.Millisecond)
		}(i)
	}

	fmt.Println("\n2Ô∏è‚É£ Setelah membuat 1000 goroutines:")
	var m2 runtime.MemStats
	runtime.ReadMemStats(&m2)
	goroutines2 := runtime.NumGoroutine()
	fmt.Printf("   Memory: %d KB, Goroutines: %d\n", m2.Alloc/1024, goroutines2)
	fmt.Printf("   üìà Memory increase: %d KB\n", (m2.Alloc-m1.Alloc)/1024)
	fmt.Printf("   üìà Goroutines increase: %d\n", goroutines2-goroutines1)

	wg.Wait()

	fmt.Println("\n3Ô∏è‚É£ Setelah semua goroutines selesai:")
	var m3 runtime.MemStats
	runtime.ReadMemStats(&m3)
	goroutines3 := runtime.NumGoroutine()
	fmt.Printf("   Memory: %d KB, Goroutines: %d\n", m3.Alloc/1024, goroutines3)

	fmt.Println(`
üí° OBSERVATIONS:
‚Ä¢ Setiap goroutine menggunakan ~8KB stack memory
‚Ä¢ 1000 goroutines = ~8MB overhead minimal
‚Ä¢ Scheduler overhead meningkat dengan jumlah goroutines
‚Ä¢ Context switching cost meningkat drastis
`)
}

// DEMO: Optimal Worker Count
func demonstrateOptimalWorkerCount() {
	fmt.Println("\nüéØ DEMO: MENCARI OPTIMAL WORKER COUNT")
	fmt.Println("=" + fmt.Sprintf("%50s", "="))

	numJobs := 1000
	workersToTest := []int{1, 2, 4, 8, 16, 32, 64, 128}

	fmt.Printf("Testing dengan %d jobs, berbagai jumlah workers:\n", numJobs)
	fmt.Println("Workers | Duration | Efficiency")
	fmt.Println("--------|----------|----------")

	var bestDuration time.Duration = time.Hour
	var bestWorkers int

	for _, numWorkers := range workersToTest {
		start := time.Now()

		jobs := make(chan int, numJobs)
		var wg sync.WaitGroup

		// Start workers
		for w := 0; w < numWorkers; w++ {
			wg.Add(1)
			go func() {
				defer wg.Done()
				for range jobs {
					// Simulasi CPU-bound work
					time.Sleep(5 * time.Millisecond)
				}
			}()
		}

		// Send jobs
		for i := 0; i < numJobs; i++ {
			jobs <- i
		}
		close(jobs)

		wg.Wait()
		duration := time.Since(start)

		efficiency := "Normal"
		if duration < bestDuration {
			bestDuration = duration
			bestWorkers = numWorkers
			efficiency = "üèÜ BEST"
		} else if duration > bestDuration*2 {
			efficiency = "‚ùå Poor"
		}

		fmt.Printf("%7d | %8v | %s\n", numWorkers, duration.Round(time.Millisecond), efficiency)
	}

	fmt.Printf("\nüéØ OPTIMAL: %d workers dengan duration %v\n", bestWorkers, bestDuration.Round(time.Millisecond))
	fmt.Printf("üí° CPU Cores: %d (reference)\n", runtime.NumCPU())
}

func main() {
	fmt.Println("üè≠ WORKER POOL vs ONE GOROUTINE PER JOB")
	fmt.Println("=======================================")

	fmt.Println(`
ü§î PERTANYAAN: 
Mengapa tidak menggunakan N goroutines untuk N jobs?
Mengapa perlu Worker Pool pattern?

Mari kita buktikan dengan benchmark!
`)

	// Demo 1: Perbandingan langsung
	numJobs := 50
	oneGoroutinePerJob(numJobs)
	workerPoolPattern(numJobs, 5)

	// Demo 2: Scalability issues
	demonstrateScalabilityIssues()

	// Demo 3: Resource management
	demonstrateResourceManagement()

	// Demo 4: Optimal worker count
	demonstrateOptimalWorkerCount()

	fmt.Println(`
üéì KESIMPULAN - MENGAPA WORKER POOL LEBIH BAIK:

1Ô∏è‚É£ MEMORY EFFICIENCY:
   ‚Ä¢ Goroutine overhead: ~8KB per goroutine
   ‚Ä¢ 1000 jobs = 1000 goroutines = ~8MB minimum
   ‚Ä¢ Worker pool: Fixed overhead, scalable

2Ô∏è‚É£ SCHEDULER OVERHEAD:
   ‚Ä¢ Go scheduler harus manage banyak goroutines
   ‚Ä¢ Context switching cost meningkat
   ‚Ä¢ Thrashing pada high concurrency

3Ô∏è‚É£ RESOURCE LIMITS:
   ‚Ä¢ OS memiliki limit untuk threads/processes
   ‚Ä¢ Memory fragmentation
   ‚Ä¢ CPU cache misses

4Ô∏è‚É£ CONTROL & MONITORING:
   ‚Ä¢ Bounded concurrency (tidak unlimited)
   ‚Ä¢ Predictable resource usage
   ‚Ä¢ Better debugging dan profiling

5Ô∏è‚É£ BACKPRESSURE:
   ‚Ä¢ Natural flow control
   ‚Ä¢ Prevents system overload
   ‚Ä¢ Graceful degradation

üöÄ BEST PRACTICE:
Workers ‚âà CPU cores untuk CPU-bound tasks
Workers ‚âà 2-4x CPU cores untuk I/O-bound tasks
`)
}
